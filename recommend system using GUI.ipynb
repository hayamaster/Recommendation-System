{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74115630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import ast as ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "window=tk.Tk()\n",
    "\n",
    "window.title(\"Movie recommendation system\")\n",
    "window.resizable(False, False)\n",
    "\n",
    "### Collaborative Filtering\n",
    "## Euclidean distance (item-based)\n",
    "def euclidean(data, title):\n",
    "    # movie title로 movieId 가져오기\n",
    "    index = df_cos[df_cos['title'] == title].index\n",
    "    movie_id= list(df_cos.loc[index, 'movieId'])[0]\n",
    "    # simiality function\n",
    "    def sim_distance(data, n1, n2):  \n",
    "        sum = 0\n",
    "        # i 값은 df_euclidean 데이터셋의 선택한 row(movieId)중 평점값이 >=0 값들의 index(userId)\n",
    "        for i in data.loc[n1, data.loc[n1, :]>=0].index:\n",
    "            # n2 = input movieId와 값이 다른 movieId\n",
    "            if data.loc[n2, i]>=0:\n",
    "                sum += math.pow(data.loc[n1, i] - data.loc[n2, i], 2)\n",
    "        return math.sqrt(1/(sum+1)) # return similarity value\n",
    "    \n",
    "    def top_match(data, movieId, rank):\n",
    "        simList = []\n",
    "        # i값은 df_euclidean 데이터셋 절반 중에서의 index(movieId)\n",
    "        # data값이 많아 시간을 줄이기 위해 전체 중 절반만 사용\n",
    "        for i in data.index[-len(data):]:\n",
    "            # input movieId 와 값이 다른 movieId의 similarity값을 simList에 append\n",
    "            if movieId != i:\n",
    "                simList.append((sim_distance(data, movieId, i), i))\n",
    "        simList.sort(reverse=True)\n",
    "        return simList[:rank] # (similarity, movieId) 리스트를 return\n",
    "    \n",
    "    def recommendation(data, movie_id):\n",
    "        res = top_match(data, movie_id, len(data))\n",
    "        score_dic = {}\n",
    "        sim_dic = {}\n",
    "        myList = []\n",
    "        for sim, mv in res:\n",
    "            # similarity >= 0을 때만 실행\n",
    "            if sim < 0:\n",
    "                continue\n",
    "            for movie in data.loc[movie_id, data.loc[movie_id, :] < 0].index:\n",
    "                simSum = 0\n",
    "                if data.loc[mv, movie] >= 0:\n",
    "                    simSum += sim * data.loc[mv, movie]\n",
    "                    score_dic.setdefault(movie, 0)\n",
    "                    score_dic[movie] += simSum\n",
    "                    sim_dic.setdefault(movie, 0)\n",
    "                    sim_dic[movie] += sim\n",
    "        for key in score_dic:\n",
    "            myList.append((score_dic[key] / sim_dic[key], key))\n",
    "        myList.sort(reverse=True)\n",
    "        return myList\n",
    "    # 추천 점수가 가장 높은 순으로 예상평점과 영화제목을 추천 (10개까지)\n",
    "    movieList = []\n",
    "    for rate, m_id in recommendation(data, movie_id):\n",
    "        if list(df_movies.loc[df_movies['movieId']==m_id, 'title']) == []:\n",
    "            continue\n",
    "        movieList.append((rate, df_movies.loc[df_movies['movieId']==m_id, 'title'].values[0]))\n",
    "    return \"- Collaborative Filtering\\n- Euclidean distance similarity\\n\"+ pd.DataFrame(movieList[:10], columns=['Rating', 'Title']).to_string()\n",
    "\n",
    "\n",
    "## Cosine similarity (CF item-based)\n",
    "def cos_item(data, title):\n",
    "    # cosine similarity\n",
    "    sim_rate = cosine_similarity(data, data)\n",
    "    sim_rate_df = pd.DataFrame(data=sim_rate, index=data.index, columns=data.index)\n",
    "    sim_rate_df = pd.DataFrame(sim_rate_df[title].sort_values(ascending=False)[1:11]).reset_index()\n",
    "    sim_rate_df.columns = ['Title', 'Cosine Similarity']\n",
    "    sim_rate_df = sim_rate_df[['Cosine Similarity', 'Title']]\n",
    "    return \"- Collaborative Filtering\\n- Cosine simiarity (item-based)\\n\" + sim_rate_df.to_string()\n",
    "    \n",
    "\n",
    "## Matrix Factorization\n",
    "# *parameters mf(data, userId)\n",
    "def mf(df_mat, user_id):\n",
    "    # convert pivot_table dataset to numpy matrix\n",
    "    matrix = df_mat.to_numpy()\n",
    "    rating_mean = np.mean(matrix, axis=1)  # user's mean rating\n",
    "    matrix_mean = matrix - rating_mean.reshape(-1, 1)  # 사용자-영화에 대해 사용자평균 뺀 값\n",
    "    # get U matrix, sigma matrix, Vt transposed matrix from 'svds' meaning 'Truncated SVD'\n",
    "    U, sigma, Vt = svds(matrix_mean, k = 12)\n",
    "    sigma = np.diag(sigma)\n",
    "    # recover original matrix\n",
    "    # dot(U, sigma, Vt) + user's mean rating\n",
    "    svd_ratings = np.dot(np.dot(U, sigma), Vt) + rating_mean.reshape(-1, 1)\n",
    "    df_svd = pd.DataFrame(svd_ratings, columns = df_mat.columns)\n",
    "\n",
    "    def recommendation(data, userId, ori_movie, ori_rating):\n",
    "        # 현재는 index로 적용되어 있어 userId-1\n",
    "        user_row_num = userId - 1\n",
    "        sorted_pre = data.iloc[user_row_num].sort_values(ascending=False)\n",
    "        # abstract datas with same 'userId's from original ratings dataset\n",
    "        user_data = ori_rating[ori_rating.userId == userId]\n",
    "        user_history = user_data.merge(ori_movie, on='movieId').sort_values(['rating'], ascending=False)\n",
    "        user_history = user_history[['userId', 'movieId', 'rating']]\n",
    "        # abstract datas without movie datas users have seen already from original movies dataset\n",
    "        recommendations = ori_movie[~ori_movie['movieId'].isin(user_history['movieId'])]\n",
    "        recommendations = recommendations.merge(pd.DataFrame(sorted_pre).reset_index(), on='movieId')\n",
    "        recommendations = recommendations.rename(columns = {user_row_num: 'Predictions'}).sort_values('Predictions', ascending=False)\n",
    "        recommendations = recommendations[['movieId', 'title', 'Predictions']]\n",
    "\n",
    "        return user_history, recommendations\n",
    "\n",
    "    already_rated, predictions = recommendation(df_svd, user_id, df_movies, df_ratings)\n",
    "#     print(\"User's history\")\n",
    "#     print(already_rated.head(10))\n",
    "    predictions = predictions[['Predictions', 'title']]\n",
    "    predictions.columns = ['Predictions rate', 'Title']\n",
    "    return \"- Collaborative Filtering\\n- Matrix Factorization (SVD)\\n\" + predictions[:10].reset_index(drop=True).to_string()\n",
    "    \n",
    "\n",
    "### Contant Based Filtering\n",
    "## Cosine similarity (CBF)\n",
    "def content_based(movie_data, target_name):\n",
    "    movie_data =  movie_data.loc[movie_data['original_language'] == 'en', :]\n",
    "    movie_data = movie_data[['id', 'title', 'original_language', 'genres']]\n",
    "\n",
    "    movie_data.id = movie_data.id.astype(int)\n",
    "    movie_data['genres'] = movie_data['genres'].apply(literal_eval)\n",
    "    movie_data['genres'] = movie_data['genres'].apply(lambda x : [d['name'] for d in x]).apply(lambda x : \" \".join(x))\n",
    "\n",
    "    tfidf_vector = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vector.fit_transform(movie_data['genres']).toarray()\n",
    "    tfidf_matrix_feature = tfidf_vector.get_feature_names()\n",
    "\n",
    "    tfidf_matrix = pd.DataFrame(tfidf_matrix, columns=tfidf_matrix_feature, index = movie_data.title)\n",
    "\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    cosine_sim_df = pd.DataFrame(cosine_sim, index = movie_data.title, columns = movie_data.title)\n",
    "\n",
    "    def genre_recommendations(target_title, matrix, items, k=10):\n",
    "        recom_idx = matrix.loc[:, target_title].values.reshape(1, -1).argsort()[:, ::-1].flatten()[1:k+1]\n",
    "        recom_title = items.iloc[recom_idx, :].title.values\n",
    "        recom_genre = items.iloc[recom_idx, :].genres.values\n",
    "        target_title_list = np.full(len(range(k)), target_title)\n",
    "        target_genre_list = np.full(len(range(k)), items[items.title == target_title].genres.values)\n",
    "        d = {\n",
    "            'target_title':target_title_list,\n",
    "            'target_genre':target_genre_list,\n",
    "            'recom_title' : recom_title,\n",
    "            'recom_genre' : recom_genre\n",
    "        }\n",
    "        return pd.DataFrame(d)\n",
    "    return \"- Content Based Filtering\\n- Cosine Similarity (Genre)\\n\" + genre_recommendations(target_name, cosine_sim_df, movie_data).to_string()\n",
    "\n",
    "### Association Rule Mining\n",
    "## Apriori\n",
    "def apriori(data, name, min_sup=0.08):    \n",
    "    from mlxtend.frequent_patterns import association_rules\n",
    "    from mlxtend.frequent_patterns import apriori\n",
    "    frequent_itemset = apriori(data, min_support=min_sup, use_colnames=True)\n",
    "\n",
    "    rules = association_rules(frequent_itemset, metric='lift', min_threshold=1)\n",
    "    rules.sort_values(by=['lift'], ascending=False, inplace=True)\n",
    "    \n",
    "    df_res = rules[rules['antecedents'].apply(lambda x: len(x) == 1 and next(iter(x)) == name)]\n",
    "    df_res = df_res[df_res['lift'] > 2]\n",
    "    movies = df_res['consequents'].values\n",
    "\n",
    "    movieList = []\n",
    "    for movie in movies:\n",
    "        lift = df_res.loc[df_res['consequents']==movie, ['lift']].values\n",
    "        for title in movie:\n",
    "            if title not in movieList:\n",
    "                movieList.append((round(lift[0][0], 4), title))\n",
    "    return \"- Association Rule Mining\\n- Apriori\\n\" + pd.DataFrame(movieList, columns=['Lift rate', 'Title']).drop_duplicates(['Title']).reset_index(drop=True)[:10].to_string()\n",
    "\n",
    "def prepare():\n",
    "    # Read two datasets\n",
    "    global movies_ori\n",
    "    movies_ori = pd.read_csv(\"movies_metadata.csv\", encoding='UTF-8')\n",
    "    ratings_ori = pd.read_csv(\"ratings_small.csv\")\n",
    "\n",
    "    ## Preprocessing (Clean the datasets)\n",
    "    # Missing value\n",
    "    # remove records of movies without title.\n",
    "    title_mask = movies_ori['title'].isna()\n",
    "    global df_movies\n",
    "    df_movies = movies_ori.loc[title_mask == False]\n",
    "    # remove records of movies with wrong 'id'\n",
    "    movieId_mask = df_movies.index[[df_movies['id'].str.contains('-')]].tolist()\n",
    "    df_movies.drop(movieId_mask, inplace=True)\n",
    "    # fill 'NaN' value of all records of columns 'overview' and 'tagline' with ''\n",
    "    df_movies['overview'] = df_movies['overview'].fillna('')\n",
    "    df_movies['tagline'] = df_movies['tagline'].fillna('')\n",
    "    # convert values of 'id' datatype str to int\n",
    "    df_movies['id'] = pd.to_numeric(df_movies['id'])\n",
    "    # convert values of 'genres' datatype dict to str, and abstract values\n",
    "    df_movies['genres'] = df_movies['genres'].apply(ast.literal_eval)\n",
    "    df_movies['genres'] = df_movies['genres'].apply(lambda x:[d['name'] for d in x]).apply(lambda x:' '.join(x))\n",
    "    # change column name 'id' to 'movieId' & 'tagline' to 'keywords'\n",
    "    df_movies.rename(columns={'id':'movieId'}, inplace=True) \n",
    "    df_movies.rename(columns={'tagline':'keywords'}, inplace=True)\n",
    "    # Features selection\n",
    "    df_movies = df_movies[['movieId', 'title', 'overview', 'genres', 'keywords', 'vote_average', 'vote_count']]\n",
    "    global df_ratings\n",
    "    df_ratings = ratings_ori.iloc[:, :-1]\n",
    "    # Merge two datasets 'df_movies' & 'df_ratings'\n",
    "    df = pd.merge(df_ratings, df_movies[['movieId', 'title']], on='movieId')\n",
    "\n",
    "\n",
    "    ## Datasets for each model and preprocessing\n",
    "    # dataset for euclidean distance\n",
    "    global df_euclidean\n",
    "    df_euclidean = df.pivot_table(index='movieId', columns='userId', values='rating').fillna(-1)\n",
    "    # dataset for cosine similarity (CBF)\n",
    "    global df_cos\n",
    "    df_cos = df_movies[['movieId', 'overview', 'title']].head(30000)\n",
    "    df_cos = df_cos.reset_index(drop=True)\n",
    "    # dataset for cosine similarity (CF item-based)\n",
    "    global df_cos_item\n",
    "    df_cos_item = df.pivot_table(index='title', columns='userId', values='rating').fillna(0)\n",
    "    # dataset for matrix factorization\n",
    "    global df_mf\n",
    "    df_mf = df.pivot_table(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "    # dataset for apriori\n",
    "    def encode_ratings(x):  \n",
    "        if x <= 0: \n",
    "            return 0\n",
    "        return 1\n",
    "    global df_apriori\n",
    "    df_apriori = df.drop_duplicates(['userId', 'title']) # drop duplicated values in 'userId' & 'title'\n",
    "    df_apriori = df_apriori.pivot_table(index='userId', columns='title', values='rating').fillna(0).astype('int64')\n",
    "    df_apriori = df_apriori.applymap(encode_ratings)  # encoding values to 0 or 1\n",
    "    # dataset for content based filtering\n",
    "    df_cbf = df_movies\n",
    "\n",
    "    resultListBox.delete(0, tk.END)\n",
    "    processButton[\"state\"] = tk.NORMAL\n",
    "\n",
    "def process():\n",
    "    resultListBox.delete(0, tk.END)\n",
    "    inputString = entryMovieName.get()\n",
    "    outputString = []\n",
    "\n",
    "    if inputString == \"\":\n",
    "        # Print error\n",
    "        resultListBox.insert(0, \"[Error] Invalid input\")\n",
    "        processButton[\"state\"] = tk.NORMAL\n",
    "        return\n",
    "\n",
    "    outputString = apriori(df_apriori, inputString).split(\"\\n\") + content_based(movies_ori, inputString).split(\"\\n\") + euclidean(df_euclidean, inputString).split(\"\\n\") + mf(df_mf, 86).split(\"\\n\")\n",
    "\n",
    "    # Print result\n",
    "    for (i, result) in enumerate(outputString):\n",
    "        resultListBox.insert(i, result)\n",
    "\n",
    "entryMovieName = tk.Entry(window, width=150)\n",
    "entryMovieName.pack()\n",
    "\n",
    "processButton = tk.Button(window, text=\"Process\", overrelief=\"solid\", width=15, command=process, repeatdelay=1000, repeatinterval=100)\n",
    "processButton[\"state\"] = tk.DISABLED\n",
    "processButton.pack()\n",
    "\n",
    "frame = tk.Frame()\n",
    "frame.pack()\n",
    "\n",
    "resultListBox = tk.Listbox(frame, selectmode='extended', width=150, height=30, activestyle='none')\n",
    "resultListBox.insert(0, \"Please wait...\")\n",
    "resultListBox.pack(side=\"left\")\n",
    "scrollbar = tk.Scrollbar(frame, orient=\"vertical\")\n",
    "scrollbar.config(command=resultListBox.yview)\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "resultListBox.config(yscrollcommand=scrollbar.set)\n",
    "\n",
    "window.after(2000, prepare)\n",
    "window.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
